# Reinforcement Learning - Grid World avec Value Iteration

4 programmes progressifs d√©montrant l'apprentissage par renforcement dans un environnement Grid World.

## üìã Programmes

### Programme 1: Agent Random (`prog1_random.py`)
Agent qui explore al√©atoirement jusqu'√† trouver le goal.
- Exploration pure sans apprentissage
- Visualisation en temps r√©el

**Ex√©cution:**
```bash
python prog1_random.py
```

### Programme 2: Value Iteration (`prog2_value_iteration.py`)
Agent intelligent utilisant Value Iteration pour apprendre la politique optimale.
- Algorithme de Value Iteration
- Affichage des Value States (couleurs)
- Affichage de la Politique Optimale (fl√®ches)
- Chemin optimal garanti

**Ex√©cution:**
```bash
python prog2_value_iteration.py
```

### Programme 3: Goal Mobile entre √âpisodes (`prog3_goal_between_episodes.py`)
Value Iteration avec goal qui change de position entre chaque √©pisode.
- R√©-entra√Ænement √† chaque √©pisode
- Adaptation √† diff√©rentes positions de goal
- Visualisation de l'apprentissage continu

**Ex√©cution:**
```bash
python prog3_goal_between_episodes.py
```

### Programme 4: Goal Mobile en Temps R√©el (`prog4_goal_during_episode.py`)
Value Iteration avec goal qui se d√©place PENDANT l'√©pisode.
- Re-planning dynamique
- Goal mobile pendant que l'agent se d√©place
- D√©fi d'apprentissage le plus complexe

**Ex√©cution:**
```bash
python prog4_goal_during_episode.py
```

## üöÄ Installation

```bash
pip install numpy matplotlib gymnasium
```

## üìä Configurations Disponibles

- **SMALL**: 5x5, 1 goal, 1 obstacle (recommand√© pour d√©mo)
- **DEFAULT**: 10x10, 1 goal, 2 obstacles
- **LARGE**: 15x15, 2 goals, 6 obstacles
- **COMPLEX**: 12x12, 3 goals, 10 obstacles

## üéØ Concepts Cl√©s

### Value Iteration
Algorithme de programmation dynamique qui calcule la valeur optimale de chaque √©tat:
```
V(s) = max_a [R(s,a) + Œ≥ √ó V(s')]
```

### Politique Optimale
Meilleure action √† prendre dans chaque √©tat pour maximiser la r√©compense cumulative.

### Visualisation
- üé® **Couleurs**: Value States (rouge‚Üívert = faible‚Üí√©lev√©)
- ‚û°Ô∏è **Fl√®ches**: Direction optimale
- üîµ **Agent**: Position actuelle
- üü° **Goal**: Objectif
- ‚¨õ **Obstacles**: Cases bloqu√©es

## üìà R√©sultats Typiques

### Programme 1 (Random)
- Taux de succ√®s: 20-40%
- Steps: 40-100 (al√©atoire)

### Programme 2 (Value Iteration)
- Taux de succ√®s: 100%
- Steps: Optimal (chemin le plus court)

### Programme 3 (Goal mobile - √©pisodes)
- Taux de succ√®s: 100% par √©pisode
- R√©-apprentissage rapide

### Programme 4 (Goal mobile - temps r√©el)
- Comportement adaptatif
- Re-planning continu

## üéì Auteur

Projet d√©monstratif pour GIIADS - Reinforcement Learning
